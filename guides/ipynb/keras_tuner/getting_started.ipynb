{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCS34moP_HXy"
      },
      "source": [
        "# Getting started with KerasTuner\n",
        "\n",
        "**Authors:** Luca Invernizzi, James Long, Francois Chollet, Tom O'Malley, Haifeng Jin<br>\n",
        "**Date created:** 2019/05/31<br>\n",
        "**Last modified:** 2021/10/27<br>\n",
        "**Description:** The basics of using KerasTuner to tune model hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l-0Hndzh_HX0",
        "outputId": "218bbc7a-fa14-4f9f-94d1-5fbe7ea9c89d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m923.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zrVD1CW_HX0"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "KerasTuner is a general-purpose hyperparameter tuning library. It has strong\n",
        "integration with Keras workflows, but it isn't limited to them: you could use\n",
        "it to tune scikit-learn models, or anything else. In this tutorial, you will\n",
        "see how to tune model architecture, training process, and data preprocessing\n",
        "steps with KerasTuner. Let's start from a simple example.\n",
        "\n",
        "## Tune the model architecture\n",
        "\n",
        "The first thing we need to do is writing a function, which returns a compiled\n",
        "Keras model. It takes an argument `hp` for defining the hyperparameters while\n",
        "building the model.\n",
        "\n",
        "### Define the search space\n",
        "\n",
        "In the following code example, we define a Keras model with two `Dense` layers.\n",
        "We want to tune the number of units in the first `Dense` layer. We just define\n",
        "an integer hyperparameter with `hp.Int('units', min_value=32, max_value=512, step=32)`,\n",
        "whose range is from 32 to 512 inclusive. When sampling from it, the minimum\n",
        "step for walking through the interval is 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F_-cFJdI_HX1"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            # Define the hyperparameter.\n",
        "            units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fOqmbP5_HX1"
      },
      "source": [
        "You can quickly test if the model builds successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CJOyKLph_HX2",
        "outputId": "07f54e5e-578e-4606-8029-0c90785568b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Sequential name=sequential, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import keras_tuner\n",
        "\n",
        "build_model(keras_tuner.HyperParameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bix1nUX5_HX2"
      },
      "source": [
        "There are many other types of hyperparameters as well. We can define multiple\n",
        "hyperparameters in the function. In the following code, we tune whether to\n",
        "use a `Dropout` layer with `hp.Boolean()`, tune which activation function to\n",
        "use with `hp.Choice()`, tune the learning rate of the optimizer with\n",
        "`hp.Float()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SjZRXAX0_HX2",
        "outputId": "7d6951d7-0219-492f-f012-354236fd0561",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Sequential name=sequential_1, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            # Tune number of units.\n",
        "            units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "            # Tune the activation function to use.\n",
        "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
        "        )\n",
        "    )\n",
        "    # Tune whether to use dropout.\n",
        "    if hp.Boolean(\"dropout\"):\n",
        "        model.add(layers.Dropout(rate=0.25))\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "    # Define the optimizer learning rate as a hyperparameter.\n",
        "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "build_model(keras_tuner.HyperParameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWOPgNwy_HX3"
      },
      "source": [
        "As shown below, the hyperparameters are actual values. In fact, they are just\n",
        "functions returning actual values. For example, `hp.Int()` returns an `int`\n",
        "value. Therefore, you can put them into variables, for loops, or if\n",
        "conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KZgz_Z6H_HX3",
        "outputId": "77a3e3db-607b-4759-a373-7faee131ce12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ],
      "source": [
        "hp = keras_tuner.HyperParameters()\n",
        "print(hp.Int(\"units\", min_value=32, max_value=512, step=32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ZquQuH_HX3"
      },
      "source": [
        "You can also define the hyperparameters in advance and keep your Keras code in\n",
        "a separate function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0lq0cQDE_HX4",
        "outputId": "adfae406-f397-4e93-b7f1-32847ff65c4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Sequential name=sequential_2, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "def call_existing_code(units, activation, dropout, lr):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(units=units, activation=activation))\n",
        "    if dropout:\n",
        "        model.add(layers.Dropout(rate=0.25))\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    units = hp.Int(\"units\", min_value=32, max_value=512, step=32)\n",
        "    activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "    dropout = hp.Boolean(\"dropout\")\n",
        "    lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    # call existing model-building code with the hyperparameter values.\n",
        "    model = call_existing_code(\n",
        "        units=units, activation=activation, dropout=dropout, lr=lr\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "build_model(keras_tuner.HyperParameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkPYQwFd_HX4"
      },
      "source": [
        "Each of the hyperparameters is uniquely identified by its name (the first\n",
        "argument). To tune the number of units in different `Dense` layers separately\n",
        "as different hyperparameters, we give them different names as `f\"units_{i}\"`.\n",
        "\n",
        "Notably, this is also an example of creating conditional hyperparameters.\n",
        "There are many hyperparameters specifying the number of units in the `Dense`\n",
        "layers. The number of such hyperparameters is decided by the number of layers,\n",
        "which is also a hyperparameter. Therefore, the total number of hyperparameters\n",
        "used may be different from trial to trial. Some hyperparameter is only used\n",
        "when a certain condition is satisfied. For example, `units_3` is only used\n",
        "when `num_layers` is larger than 3. With KerasTuner, you can easily define\n",
        "such hyperparameters dynamically while creating the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OgZqtaom_HX4",
        "outputId": "35379cc5-87f5-4676-de24-255e8a5ba508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Sequential name=sequential_3, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "    # Tune the number of layers.\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
        "        model.add(\n",
        "            layers.Dense(\n",
        "                # Tune number of units separately.\n",
        "                units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
        "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
        "            )\n",
        "        )\n",
        "    if hp.Boolean(\"dropout\"):\n",
        "        model.add(layers.Dropout(rate=0.25))\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "build_model(keras_tuner.HyperParameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xT68tF3_HX4"
      },
      "source": [
        "### Start the search\n",
        "\n",
        "After defining the search space, we need to select a tuner class to run the\n",
        "search. You may choose from `RandomSearch`, `BayesianOptimization` and\n",
        "`Hyperband`, which correspond to different tuning algorithms. Here we use\n",
        "`RandomSearch` as an example.\n",
        "\n",
        "To initialize the tuner, we need to specify several arguments in the initializer.\n",
        "\n",
        "* `hypermodel`. The model-building function, which is `build_model` in our case.\n",
        "* `objective`. The name of the objective to optimize (whether to minimize or\n",
        "maximize is automatically inferred for built-in metrics). We will introduce how\n",
        "to use custom metrics later in this tutorial.\n",
        "* `max_trials`. The total number of trials to run during the search.\n",
        "* `executions_per_trial`. The number of models that should be built and fit for\n",
        "each trial. Different trials have different hyperparameter values. The\n",
        "executions within the same trial have the same hyperparameter values. The\n",
        "purpose of having multiple executions per trial is to reduce results variance\n",
        "and therefore be able to more accurately assess the performance of a model. If\n",
        "you want to get results faster, you could set `executions_per_trial=1` (single\n",
        "round of training for each model configuration).\n",
        "* `overwrite`. Control whether to overwrite the previous results in the same\n",
        "directory or resume the previous search instead. Here we set `overwrite=True`\n",
        "to start a new search and ignore any previous results.\n",
        "* `directory`. A path to a directory for storing the search results.\n",
        "* `project_name`. The name of the sub-directory in the `directory`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6Ik6kCdT_HX4"
      },
      "outputs": [],
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    executions_per_trial=2,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"helloworld\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMxK4uMo_HX5"
      },
      "source": [
        "You can print a summary of the search space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R3pTL6Xv_HX5",
        "outputId": "ead5264d-4c46-44d3-ebed-8e607f96e1bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 5\n",
            "num_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': 'linear'}\n",
            "units_0 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "activation (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
            "dropout (Boolean)\n",
            "{'default': False, 'conditions': []}\n",
            "lr (Float)\n",
            "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ],
      "source": [
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0YzkU8-_HX5"
      },
      "source": [
        "Before starting the search, let's prepare the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V4HfF2_t_HX5",
        "outputId": "fa58e7b6-f475-44dc-fb99-8698194ddaa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x[:-10000]\n",
        "x_val = x[-10000:]\n",
        "y_train = y[:-10000]\n",
        "y_val = y[-10000:]\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
        "x_val = np.expand_dims(x_val, -1).astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pQffr81_HX5"
      },
      "source": [
        "Then, start the search for the best hyperparameter configuration.\n",
        "All the arguments passed to `search` is passed to `model.fit()` in each\n",
        "execution. Remember to pass `validation_data` to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PF3V_yH4_HX5",
        "outputId": "af9d73c2-25d5-40df-ed79-09d9083cc5a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 00m 19s]\n",
            "val_accuracy: 0.9569500088691711\n",
            "\n",
            "Best val_accuracy So Far: 0.9695999920368195\n",
            "Total elapsed time: 00h 02m 50s\n"
          ]
        }
      ],
      "source": [
        "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBuuubW-_HX6"
      },
      "source": [
        "During the `search`, the model-building function is called with different\n",
        "hyperparameter values in different trial. In each trial, the tuner would\n",
        "generate a new set of hyperparameter values to build the model. The model is\n",
        "then fit and evaluated. The metrics are recorded. The tuner progressively\n",
        "explores the space and finally finds a good set of hyperparameter values.\n",
        "\n",
        "### Query the results\n",
        "\n",
        "When search is over, you can retrieve the best model(s). The model is saved at\n",
        "its best performing epoch evaluated on the `validation_data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ep85G_ro_HX6",
        "outputId": "2f1072b1-2000-41ba-d0f6-1772c2d17d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m16,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m330\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m418,666\u001b[0m (1.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">418,666</span> (1.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m418,666\u001b[0m (1.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">418,666</span> (1.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get the top 2 models.\n",
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI7I7TWx_HX6"
      },
      "source": [
        "You can also print a summary of the search results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "79dotL1A_HX6",
        "outputId": "ed2a639a-65b1-410e-bac1-148f91f7f918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in my_dir/helloworld\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 0 summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "units_0: 512\n",
            "activation: relu\n",
            "dropout: True\n",
            "lr: 0.0004404944375164228\n",
            "units_1: 32\n",
            "Score: 0.9695999920368195\n",
            "\n",
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "num_layers: 3\n",
            "units_0: 416\n",
            "activation: relu\n",
            "dropout: True\n",
            "lr: 0.0031495380852312486\n",
            "units_1: 96\n",
            "units_2: 32\n",
            "Score: 0.9656000137329102\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 32\n",
            "activation: relu\n",
            "dropout: False\n",
            "lr: 0.0017010977253402633\n",
            "units_1: 96\n",
            "units_2: 192\n",
            "Score: 0.9569500088691711\n"
          ]
        }
      ],
      "source": [
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSH59MnL_HX6"
      },
      "source": [
        "You will find detailed logs, checkpoints, etc, in the folder\n",
        "`my_dir/helloworld`, i.e. `directory/project_name`.\n",
        "\n",
        "You can also visualize the tuning results using TensorBoard and HParams plugin.\n",
        "For more information, please following\n",
        "[this link](https://keras.io/guides/keras_tuner/visualize_tuning/).\n",
        "\n",
        "### Retrain the model\n",
        "\n",
        "If you want to train the model with the entire dataset, you may retrieve the\n",
        "best hyperparameters and retrain the model by yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aQ3eotHs_HX6",
        "outputId": "21de9002-22c0-49a7-d0e9-4278d191b472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.8241 - loss: 0.5920\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7951b35e9d50>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Get the top 2 hyperparameters.\n",
        "best_hps = tuner.get_best_hyperparameters(5)\n",
        "# Build the model with the best hp.\n",
        "model = build_model(best_hps[0])\n",
        "# Fit with the entire dataset.\n",
        "x_all = np.concatenate((x_train, x_val))\n",
        "y_all = np.concatenate((y_train, y_val))\n",
        "model.fit(x=x_all, y=y_all, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUpIi90P_HX6"
      },
      "source": [
        "## Tune model training\n",
        "\n",
        "To tune the model building process, we need to subclass the `HyperModel` class,\n",
        "which also makes it easy to share and reuse hypermodels.\n",
        "\n",
        "We need to override `HyperModel.build()` and `HyperModel.fit()` to tune the\n",
        "model building and training process respectively. A `HyperModel.build()`\n",
        "method is the same as the model-building function, which creates a Keras model\n",
        "using the hyperparameters and returns it.\n",
        "\n",
        "In `HyperModel.fit()`, you can access the model returned by\n",
        "`HyperModel.build()`,`hp` and all the arguments passed to `search()`. You need\n",
        "to train the model and return the training history.\n",
        "\n",
        "In the following code, we will tune the `shuffle` argument in `model.fit()`.\n",
        "\n",
        "It is generally not needed to tune the number of epochs because a built-in\n",
        "callback is passed to `model.fit()` to save the model at its best epoch\n",
        "evaluated by the `validation_data`.\n",
        "\n",
        "> **Note**: The `**kwargs` should always be passed to `model.fit()` because it\n",
        "contains the callbacks for model saving and tensorboard plugins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9I2YYYpS_HX6"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MyHyperModel(keras_tuner.HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = keras.Sequential()\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(\n",
        "            layers.Dense(\n",
        "                units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "                activation=\"relu\",\n",
        "            )\n",
        "        )\n",
        "        model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "        model.compile(\n",
        "            optimizer=\"adam\",\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def fit(self, hp, model, *args, **kwargs):\n",
        "        return model.fit(\n",
        "            *args,\n",
        "            # Tune whether to shuffle the data in each epoch.\n",
        "            shuffle=hp.Boolean(\"shuffle\"),\n",
        "            **kwargs,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evWi0YfH_HX7"
      },
      "source": [
        "Again, we can do a quick check to see if the code works correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GVyCaviq_HX7",
        "outputId": "10d26872-d837-4476-9a58-2e3dad98e4e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.0799 - loss: 12.8186\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7951b3a60a10>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "hp = keras_tuner.HyperParameters()\n",
        "hypermodel = MyHyperModel()\n",
        "model = hypermodel.build(hp)\n",
        "hypermodel.fit(hp, model, np.random.rand(100, 28, 28), np.random.rand(100, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnxiAAbS_HX7"
      },
      "source": [
        "## Tune data preprocessing\n",
        "\n",
        "To tune data preprocessing, we just add an additional step in\n",
        "`HyperModel.fit()`, where we can access the dataset from the arguments. In the\n",
        "following code, we tune whether to normalize the data before training the\n",
        "model. This time we explicitly put `x` and `y` in the function signature\n",
        "because we need to use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AP7R92oM_HX7",
        "outputId": "c713ae83-c828-4b3d-f946-84941a9c57f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.0832 - loss: 12.5962\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7951b3b8bc90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\n",
        "class MyHyperModel(keras_tuner.HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = keras.Sequential()\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(\n",
        "            layers.Dense(\n",
        "                units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "                activation=\"relu\",\n",
        "            )\n",
        "        )\n",
        "        model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "        model.compile(\n",
        "            optimizer=\"adam\",\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def fit(self, hp, model, x, y, **kwargs):\n",
        "        if hp.Boolean(\"normalize\"):\n",
        "            x = layers.Normalization()(x)\n",
        "        return model.fit(\n",
        "            x,\n",
        "            y,\n",
        "            # Tune whether to shuffle the data in each epoch.\n",
        "            shuffle=hp.Boolean(\"shuffle\"),\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "\n",
        "hp = keras_tuner.HyperParameters()\n",
        "hypermodel = MyHyperModel()\n",
        "model = hypermodel.build(hp)\n",
        "hypermodel.fit(hp, model, np.random.rand(100, 28, 28), np.random.rand(100, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umX3hIdG_HX7"
      },
      "source": [
        "If a hyperparameter is used both in `build()` and `fit()`, you can define it in\n",
        "`build()` and use `hp.get(hp_name)` to retrieve it in `fit()`. We use the\n",
        "image size as an example. It is both used as the input shape in `build()`, and\n",
        "used by data prerprocessing step to crop the images in `fit()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "208Plihd_HX7",
        "outputId": "48fea43c-dfd9-4fc9-fb1e-49a92bf47214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 00m 22s]\n",
            "val_accuracy: 0.972599983215332\n",
            "\n",
            "Best val_accuracy So Far: 0.972599983215332\n",
            "Total elapsed time: 00h 01m 03s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class MyHyperModel(keras_tuner.HyperModel):\n",
        "    def build(self, hp):\n",
        "        image_size = hp.Int(\"image_size\", 10, 28)\n",
        "        inputs = keras.Input(shape=(image_size, image_size))\n",
        "        outputs = layers.Flatten()(inputs)\n",
        "        outputs = layers.Dense(\n",
        "            units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "            activation=\"relu\",\n",
        "        )(outputs)\n",
        "        outputs = layers.Dense(10, activation=\"softmax\")(outputs)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(\n",
        "            optimizer=\"adam\",\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def fit(self, hp, model, x, y, validation_data=None, **kwargs):\n",
        "        if hp.Boolean(\"normalize\"):\n",
        "            x = layers.Normalization()(x)\n",
        "        image_size = hp.get(\"image_size\")\n",
        "        cropped_x = x[:, :image_size, :image_size, :]\n",
        "        if validation_data:\n",
        "            x_val, y_val = validation_data\n",
        "            cropped_x_val = x_val[:, :image_size, :image_size, :]\n",
        "            validation_data = (cropped_x_val, y_val)\n",
        "        return model.fit(\n",
        "            cropped_x,\n",
        "            y,\n",
        "            # Tune whether to shuffle the data in each epoch.\n",
        "            shuffle=hp.Boolean(\"shuffle\"),\n",
        "            validation_data=validation_data,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "\n",
        "tuner = keras_tuner.RandomSearch(\n",
        "    MyHyperModel(),\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"tune_hypermodel\",\n",
        ")\n",
        "\n",
        "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITuM_DGY_HX7"
      },
      "source": [
        "### Retrain the model\n",
        "\n",
        "Using `HyperModel` also allows you to retrain the best model by yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e6dJqBtG_HX7",
        "outputId": "5f46c035-556b-4e58-9b80-06c0f334e041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8956 - loss: 0.3671\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7951b2471290>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "hypermodel = MyHyperModel()\n",
        "best_hp = tuner.get_best_hyperparameters()[0]\n",
        "model = hypermodel.build(best_hp)\n",
        "hypermodel.fit(best_hp, model, x_all, y_all, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKQvWJAc_HX7"
      },
      "source": [
        "## Specify the tuning objective\n",
        "\n",
        "In all previous examples, we all just used validation accuracy\n",
        "(`\"val_accuracy\"`) as the tuning objective to select the best model. Actually,\n",
        "you can use any metric as the objective. The most commonly used metric is\n",
        "`\"val_loss\"`, which is the validation loss.\n",
        "\n",
        "### Built-in metric as the objective\n",
        "\n",
        "There are many other built-in metrics in Keras you can use as the objective.\n",
        "Here is [a list of the built-in metrics](https://keras.io/api/metrics/).\n",
        "\n",
        "To use a built-in metric as the objective, you need to follow these steps:\n",
        "\n",
        "* Compile the model with the the built-in metric. For example, you want to use\n",
        "`MeanAbsoluteError()`. You need to compile the model with\n",
        "`metrics=[MeanAbsoluteError()]`. You may also use its name string instead:\n",
        "`metrics=[\"mean_absolute_error\"]`. The name string of the metric is always\n",
        "the snake case of the class name.\n",
        "\n",
        "* Identify the objective name string. The name string of the objective is\n",
        "always in the format of `f\"val_{metric_name_string}\"`. For example, the\n",
        "objective name string of mean squared error evaluated on the validation data\n",
        "should be `\"val_mean_absolute_error\"`.\n",
        "\n",
        "* Wrap it into `keras_tuner.Objective`. We usually need to wrap the objective\n",
        "into a `keras_tuner.Objective` object to specify the direction to optimize the\n",
        "objective. For example, we want to minimize the mean squared error, we can use\n",
        "`keras_tuner.Objective(\"val_mean_absolute_error\", \"min\")`. The direction should\n",
        "be either `\"min\"` or `\"max\"`.\n",
        "\n",
        "* Pass the wrapped objective to the tuner.\n",
        "\n",
        "You can see the following barebone code example."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example with metrics=[\"mean_absolute_error\"]"
      ],
      "metadata": {
        "id": "IQNiHqt6HNlc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xLzRSedH_HYG",
        "outputId": "994d4296-7981-4643-bac6-1ea57d832e39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 00m 02s]\n",
            "val_mean_absolute_error: 0.4522615373134613\n",
            "\n",
            "Best val_mean_absolute_error So Far: 0.38685232400894165\n",
            "Total elapsed time: 00h 00m 06s\n",
            "Results summary\n",
            "Results in my_dir/built_in_metrics\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_mean_absolute_error\", direction=\"min\")\n",
            "\n",
            "Trial 0 summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "Score: 0.38685232400894165\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "Score: 0.4522615373134613\n",
            "\n",
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "Score: 0.5142424702644348\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def build_regressor(hp):\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(units=hp.Int(\"units\", 32, 128, 32), activation=\"relu\"),\n",
        "            layers.Dense(units=1),\n",
        "        ]\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"mean_squared_error\",\n",
        "        # Objective is one of the metrics.\n",
        "        metrics=[keras.metrics.MeanAbsoluteError()],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_regressor,\n",
        "    # The objective name and direction.\n",
        "    # Name is the f\"val_{snake_case_metric_class_name}\".\n",
        "    objective=keras_tuner.Objective(\"val_mean_absolute_error\", direction=\"min\"),\n",
        "    max_trials=3,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"built_in_metrics\",\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x=np.random.rand(100, 10),\n",
        "    y=np.random.rand(100, 1),\n",
        "    validation_data=(np.random.rand(20, 10), np.random.rand(20, 1)),\n",
        ")\n",
        "\n",
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example with metrics=[\"root_mean_squared_error\"]"
      ],
      "metadata": {
        "id": "7-JZCT9LHXVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_regressor(hp):\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(units=hp.Int(\"units\", 32, 128, 32), activation=\"relu\"),\n",
        "            layers.Dense(units=1),\n",
        "        ]\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"mean_squared_error\",\n",
        "        # Objective is one of the metrics.\n",
        "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_regressor,\n",
        "    # The objective name and direction.\n",
        "    # Name is the f\"val_{snake_case_metric_class_name}\".\n",
        "    objective=keras_tuner.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n",
        "    max_trials=3,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"built_in_metrics\",\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x=np.random.rand(100, 10),\n",
        "    y=np.random.rand(100, 1),\n",
        "    validation_data=(np.random.rand(20, 10), np.random.rand(20, 1)),\n",
        ")\n",
        "\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "x86i6IfwHWwr",
        "outputId": "f8af8c39-76e5-4bec-cc36-5f2e78fcd857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 00m 03s]\n",
            "val_root_mean_squared_error: 0.566635251045227\n",
            "\n",
            "Best val_root_mean_squared_error So Far: 0.452985018491745\n",
            "Total elapsed time: 00h 00m 08s\n",
            "Results summary\n",
            "Results in my_dir/built_in_metrics\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_root_mean_squared_error\", direction=\"min\")\n",
            "\n",
            "Trial 0 summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "Score: 0.452985018491745\n",
            "\n",
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "units: 128\n",
            "Score: 0.463850200176239\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "Score: 0.566635251045227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpCXSs2z_HYJ"
      },
      "source": [
        "## KerasTuner includes pre-made tunable applications: HyperResNet and HyperXception\n",
        "\n",
        "These are ready-to-use hypermodels for computer vision.\n",
        "\n",
        "They come pre-compiled with `loss=\"categorical_crossentropy\"` and\n",
        "`metrics=[\"accuracy\"]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0d0E0KVu_HYK"
      },
      "outputs": [],
      "source": [
        "from keras_tuner.applications import HyperResNet\n",
        "\n",
        "hypermodel = HyperResNet(input_shape=(28, 28, 1), classes=10)\n",
        "\n",
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=2,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"built_in_hypermodel\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "getting_started",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}